from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.utils.dates import days_ago


default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 10, 21),
    'depends_on_past': False,
    'email': ['admin@admin.admin'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG('fetch_NYC_data',
          default_args=default_args,
          description='Runs an external Python script to get Taxi data',
          schedule_interval='@daily',
          catchup=False)

# Dataproc cluster details
cluster_name = "cluster-8399-testboto"
region = "us-central1"
project_id = "integrated-hawk-433114-s2"

with dag:
    run_script_task = BashOperator(
        task_id='run_script',
        bash_command='python3 /home/arnab/airflow/dags/data_extract_NYC.py',
    )
    
    clean_data_task = DataprocSubmitJobOperator(
        task_id="clean_data_task",
        job_config={
            "placement": {
                "cluster_name": cluster_name
            },
            "pyspark_job": {
                "jar_file_uris": ["gs://spark_jobs_test_boto/script/data_clean_NYC.py"],
                "properties": {
                    "spark.sql.sources.partitionOverwriteMode": "dynamic"
                },
                "args": [
                    "gs://spark_jobs_test_boto/data/yellow_tripdata_2024-01.parquet",  
                    "gs://spark_jobs_test_boto/output/cleaned_yellow_tripdata_2024-01.csv"
                ]
            }
        },
        region=region,
        project_id=project_id
    )

# Set the dependencies between the tasks

run_script_task >> clean_data_task